{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4acb620",
   "metadata": {},
   "source": [
    "## Prepare dataset for SOMS using BIOPERIANT12 CNCLNG01 1995-2009\n",
    "24/10/22 <br>\n",
    "Includes the missing files <br>\n",
    "Circularity index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ce228",
   "metadata": {},
   "source": [
    "## Eddy Detection with Okubo-Weiss\n",
    "Use ndimage to group together cyclonic and anticyclonic eddies similar to the eddy detection algorithm \n",
    "\n",
    "Instead of using ssh can use our OW masks \n",
    "\n",
    "Run through same criteria "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c719c3c",
   "metadata": {},
   "source": [
    "### List changes\n",
    "* Dates from the original code\n",
    "* load_lonlat: changed the pathroot \n",
    "* load_eta:\n",
    "    * changed pathroot**\n",
    "    * changed eta to sossheig (instead of interpolating - there is an error at 0°E and interpolation doens't need to be done, but need to tell it where to slice using find_ind\n",
    "* Change spherical law of cosines in distance matrix:\n",
    "    * due to rounding errors in np.radians it gives an error ~0.2km in the case I was seeing\n",
    "    * rounding st the 8th decimal produces this error\n",
    "    * write my own radians2 formula to do it properly\n",
    "* time_counter = 0 2007m04d01 gridT so need to remove it from okubo and from RV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80acacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import netCDF4 as nc4\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import pandas as pd\n",
    "#from glob import glob\n",
    "import glob\n",
    "\n",
    "import numpy.linalg as linalg\n",
    "import scipy.signal as signal\n",
    "import scipy.ndimage as ndimage\n",
    "import scipy.interpolate as interpolate\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/tsmith/scratch/eddyTracking/ecjoliver/')\n",
    "import params \n",
    "\n",
    "import re\n",
    "\n",
    "from scipy.io.netcdf import netcdf_file as netcdf\n",
    "import cmocean.cm as cmo\n",
    "import warnings\n",
    "import matplotlib.colors as colors\n",
    "from cartopy import crs as ccrs, feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13f69f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8e6e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Ask dask for 8 workers to distribute processing load\n",
    "# from dask.distributed import Client, progress\n",
    "# client = Client(threads_per_worker=1, n_workers=8)\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620d4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc361f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ind(grid1d, coord):\n",
    "        a=abs(grid1d-coord)\n",
    "        return np.where(a==np.min(a))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26eda3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dask? but dask is only xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e9d82",
   "metadata": {},
   "source": [
    "## Import relative vorticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c5e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_listRV = glob.glob('/home/tsmith/scratch/Okubo/BP12-CNCLNG01/depth/BIOPERIANT12-CNCLNG01_y200*m**d**_RV.nc')\n",
    "file_listRV.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed72126",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_RV = xr.open_mfdataset(file_listRV, decode_times=False,engine='netcdf4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1149db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = grid_RV['lat'][0]\n",
    "lons = grid_RV['lon'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc41e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "j1 = find_ind(lons[0,:],-11)\n",
    "j2 = find_ind(lons[0,:],15)\n",
    "i1 = find_ind(lats[:,0],-60)\n",
    "i2 = find_ind(lats[:,0],-37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9202580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RV = grid_RV['Relative_vorticity'][:,0,i1:i2+1,j1:j2+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6715607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use in circularity\n",
    "lonsRVx = grid_RV['lon'][0,i1:i2+1,j1:j2+1]\n",
    "latsRVx = grid_RV['lat'][0,i1:i2+1,j1:j2+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1788be",
   "metadata": {},
   "source": [
    "[589:1016] is -60°S to -37°S\n",
    "\n",
    "[3313:3626] is -11° to 15°E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef45c3",
   "metadata": {},
   "source": [
    "## Import okubo mask files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "249aeadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_listO=glob.glob(\"/home/tsmith/scratch/Okubo/BP12-CNCLNG01/depth/BIOPERIANT12-CNCLNG01_y200*[5-9]m**d**_Okubomask.nc\")\n",
    "file_listO.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3133e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridO = xr.open_mfdataset(file_listO, decode_times=False,engine='netcdf4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0517c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Okubo_mask = gridO['okubo_mask'][:,i1:i2+1,j1:j2+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df3a80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "RV_mask = RV.where(Okubo_mask == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a844e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make nan=0\n",
    "RV_mask = RV_mask.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fc14ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 19min 49s, total: 21min 44s\n",
      "Wall time: 25min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rv_mask = RV_mask.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef3fcfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to try go through without .values and keep as xarray\n",
    "#but will probably have to change it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8922588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rv_mask = RV_mask.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adc4c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lon_all = grid['lon'][0,0,:]\n",
    "# lat_all = grid['lat'][0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1fbd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_nemo_globber_specifytpe(exp_path,return_dates=False):\n",
    "    \"\"\"function that globs for NEMO files from the raw experiment output looking for velocity files only.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exp_path:  path to NEMO experiment (will glob through the year directories)\n",
    "    1) 'grid_T_2D' : glob all files of type grid_T_2D\n",
    "\n",
    "    return_dates:  optional. will run find_dates function before returning and so will return a pandas DataFrame\n",
    "\n",
    "    :returns: \n",
    "\n",
    "    For 'grid_T_2D' : single list of globbed files\n",
    "\n",
    "    Will return pandas DataFrame instead if return_dates=True\n",
    "\n",
    "    WARNING: if return_dates is true you will get one line for every new date in the pandas dataframe!\n",
    "    (There will be multiple references to the same file.)\n",
    "\n",
    "    Notes\n",
    "    -------\n",
    "    Runs reg_date by default.\n",
    "\n",
    "    Typical NEMO output looks like:\n",
    "    \n",
    "        cordex24-ERAI01_1d_19910101_19911231_grid_T_2D.nc\n",
    "\n",
    "    Example \n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    #_lg.info(\"We are globbing for experiment type: \" + os.path.basename(exp_path))\n",
    "    #_lg.info(\"We are globbing for the following file type: \" + file_type)\n",
    "    glob_pattern='*_gridT.nc'\n",
    "    #glob_pattern='*/*_grid_T_2D.nc'\n",
    "\n",
    "    infiles=sorted(glob.glob(exp_path + glob_pattern ))\n",
    "    assert(infiles!=[]),\"glob didn't find anything!\"\n",
    "\n",
    "    #if isinstance(date_extraction,(list,tuple)):\n",
    "        #infiles=globbed_date_remove(infiles)\n",
    "\n",
    "    if return_dates:\n",
    "        return find_dates(infiles)\n",
    "    return infiles\n",
    "\n",
    "\n",
    "def reg_date(string_to_find_date_in):\n",
    "    \"\"\"function that uses regular expressions to find start and end date from file.\n",
    "    \n",
    "    :string_to_find_date_in: string from NEMO netCDF file.\n",
    "    :returns: string with start and end date\n",
    "    \"\"\"\n",
    "    #really handonline regex finder: https://regex101.com/#python\n",
    "    exp = re.search(r'[0-9]{4}m[0-9]{2}d[0-9]{2}', string_to_find_date_in) \n",
    "    #print(exp)\n",
    "    exp=exp.group()\n",
    "    \n",
    "    #print exp\n",
    "    return exp\n",
    "\n",
    "def find_dates(globbedfiles):\n",
    "    \"\"\"function that finds dates associated with passed list of NEMO output. Returns in pandas DataFrame.\n",
    "\n",
    "    NB: assumes one day per NEMO output.\n",
    "    modified ts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    :globbedfiles: list of globbed files from raw_nemo_globber with pattern:\n",
    "        */cordex24*_1d_*_grid_T_2D.nc\n",
    "\n",
    "    :returns: dataframe of names and dates in pandas DataFrame.\n",
    "    \"\"\"\n",
    "    time_index = []\n",
    "    for file in globbedfiles:\n",
    "        dates_nemo=pd.Timestamp(int(reg_date(file)[0:4]),int(reg_date(file)[5:7]),\n",
    "                                      int(reg_date(file)[8:10]))\n",
    "        \n",
    "        time_index.append(dates_nemo)\n",
    "\n",
    "    time_index = pd.Series(time_index)\n",
    "    #might have to change file_time_index to 0 for all - because we have one file for every entry\n",
    "    #file_time_index = list(range(len(globbedfiles)))\n",
    "    file_time_index = 0\n",
    "    file_list = globbedfiles\n",
    "#     print(time_index)\n",
    "#     print(file_time_index)\n",
    "#     print(file_list)\n",
    "    nemo_df=pd.DataFrame({'date':time_index,\\\n",
    "                          'file_time_index':file_time_index,\\\n",
    "                          'file_list':file_list\\\n",
    "                          })\n",
    "\n",
    "    nemo_df.index=nemo_df.date\n",
    "    del nemo_df['date']\n",
    "    return nemo_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b592fb4",
   "metadata": {},
   "source": [
    "Will have to re-write eta code for when looping over multiple years\n",
    "\n",
    "Pathroot\n",
    "\n",
    "Problem with 2007m04d01 no time counter=0 so dropping it and redoing with T=1094\n",
    "\n",
    "Also issue with loading 2006m01d01 wrong time counter (so it is overwritten by 2006m01d03 as they are the same), but should be fine if importing files one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c82e214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    idx=(np.abs(array-value)).argmin()\n",
    "    return array[idx], idx\n",
    "\n",
    "def nanmean(array, axis=None):\n",
    "    return np.mean(np.ma.masked_array(array, np.isnan(array)), axis)\n",
    "\n",
    "def load_lonlat(run, disk='erebor'):\n",
    "    '''\n",
    "    Loads latitude and longitude vectors from OFAM runs or AVISO\n",
    "    Assumes root is /mnt/erebor unless cerberus is specified.\n",
    "    '''\n",
    "\n",
    "    #    \n",
    "    # OFAM\n",
    "    #\n",
    "    if run=='cb_NEMO':\n",
    "        #just need one file for lon/lat so its fine\n",
    "        pathroot='/mnt/nrestore/users/ERTH0834/BIOPERIANT12/BIOPERIANT12-CNCLNG01-S/LIN_INTERP/2005/'\n",
    "\n",
    "        # Find week's map\n",
    "        #file_header = '*/cordex24-ERAI01_1d_*_grid_T_2D'\n",
    "        #file_list = glob.glob(pathroot + file_header + '*.nc')\n",
    "        file_list=raw_nemo_globber_specifytpe(pathroot,return_dates=False)\n",
    "\n",
    "        assert (len(file_list)>0),\"globbing failed, exiting\"\n",
    "\n",
    "        #this is just for the lat/lon info so we only need one time step..\n",
    "\n",
    "        filename = file_list[0]\n",
    "        #print filename\n",
    "\n",
    "        fileobj = Dataset(filename,mode='r')\n",
    "        #lon = fileobj.variables['nav_lon'][:]\n",
    "        #lat = fileobj.variables['nav_lat'][:]\n",
    "        #h'm need to interpolate...\n",
    "\n",
    "#         lon=np.arange(params.lon1,params.lon2,.25)\n",
    "#         lat=np.arange(params.lat1,params.lat2,.25)\n",
    "        \n",
    "#         lon=np.arange(params.lon1,params.lon2,.083)\n",
    "#         lat=np.arange(params.lat1,params.lat2,.083)\n",
    "\n",
    "#read lon/lat straight from NEMO\n",
    "        lon_all = fileobj.variables['nav_lon'][:]\n",
    "        lat_all = fileobj.variables['nav_lat'][:]\n",
    "\n",
    "        lat = lat_all[find_ind(lat_all[:,0],params.lat1):find_ind(lat_all[:,0],params.lat2)+1,0]\n",
    "        lon = lon_all[0,find_ind(lon_all[0,:],params.lon1):find_ind(lon_all[0,:],params.lon2)+1]\n",
    "        \n",
    "        \n",
    "    return lon, lat\n",
    "\n",
    "\n",
    "def restrict_lonlat(lon, lat, lon1, lon2, lat1, lat2):\n",
    "    '''\n",
    "    Restricts latitude and longitude vectors given\n",
    "    input limits.\n",
    "    '''\n",
    "\n",
    "    tmp, i1 = find_nearest(lon, lon1)\n",
    "    tmp, i2 = find_nearest(lon, lon2)\n",
    "    tmp, j1 = find_nearest(lat, lat1)\n",
    "    tmp, j2 = find_nearest(lat, lat2)\n",
    "\n",
    "    lon = lon[i1:i2+1]\n",
    "    lat = lat[j1:j2+1]\n",
    "\n",
    "    return lon, lat, i1, i2, j1, j2\n",
    "\n",
    "\n",
    "def load_eta(run, tt, i1, i2, j1, j2, disk='erebor'):\n",
    "    '''\n",
    "    Loads sea surface height field from OFAM runs or Aviso\n",
    "    Assumes root is /mnt/erebor unless cerberus is specified.\n",
    "     run = 'CTRL' or 'A1B' or 'AVISO'\n",
    "    '''\n",
    "\n",
    "    #    \n",
    "    # OFAM\n",
    "    #\n",
    "    if run=='cb_NEMO':\n",
    "#         def nemo_fixdateline(netcdf_datasetobj):\n",
    "#             \"\"\"function to return fixed version of netcdf nav_lon variable\n",
    "            \n",
    "#             :netcdf_datasetobj: netCDF4 Dataset object of nemo file\n",
    "#             :returns: nemo_lons numpy array with fixed dateline\n",
    "#             \"\"\"\n",
    "#             nemo_lons=netcdf_datasetobj.variables['nav_lon'][:]\n",
    "#             #fix the dateline\n",
    "#             for index in np.arange(np.shape(nemo_lons)[0]):\n",
    "#                 start=np.where(np.sign(nemo_lons[index,:])==-1)[0][0]\n",
    "#                 nemo_lons[index,start:]=nemo_lons[index,start:]+360\n",
    "#             return nemo_lons\n",
    "\n",
    "        \n",
    "        \n",
    "        #pathroot='/srv/ccrc/data42/z3457920/20151012_eac_sep_dynamics/nemo_cordex24_ERAI01/'\n",
    "        #pathroot=params.pathroot\n",
    "        \n",
    "        #TS: overwriting params.pathroot \n",
    "        \n",
    "        pathroot='/mnt/nrestore/users/ERTH0834/BIOPERIANT12/BIOPERIANT12-CNCLNG01-S/LIN_INTERP/200*[5-9]/'\n",
    "        \n",
    "        \n",
    "        file_list=raw_nemo_globber_specifytpe(pathroot,return_dates=True)\n",
    "        #TS remove OST problems from gridU,V\n",
    "        #drop all missing files (other files - gridU,gridV)\n",
    "#         for i in missing:\n",
    "            \n",
    "#             file_list = file_list.drop(i)\n",
    "        \n",
    "        infile=file_list.iloc[tt]['file_list']\n",
    "        file_time_index=file_list.iloc[tt]['file_time_index']\n",
    "\n",
    "        #h'm need to interpolate because of funky NEMO grid...\n",
    "        fileobj = Dataset(infile,mode='r')\n",
    "        #print(fileobj)\n",
    "        #loni=nemo_fixdateline(fileobj)\n",
    "        #loni = fileobj.variables['nav_lon'][:]\n",
    "        \n",
    "        \n",
    "#         loni = fileobj.variables['nav_lon'][:]\n",
    "        \n",
    "#         lati = fileobj.variables['nav_lat'][:]\n",
    "\n",
    "#         lon=np.arange(params.lon1,params.lon2,.25)\n",
    "#         lat=np.arange(params.lat1,params.lat2,.25)\n",
    "        \n",
    "#         lon=np.arange(params.lon1,params.lon2,.083)\n",
    "#         lat=np.arange(params.lat1,params.lat2,.083)\n",
    "\n",
    "        loni = fileobj.variables['nav_lon'][:]\n",
    "        lati = fileobj.variables['nav_lat'][:]\n",
    "\n",
    "        lat = lati[find_ind(lati[:,0],params.lat1):find_ind(lati[:,0],params.lat2)+1,0]\n",
    "        lon = loni[0,find_ind(loni[0,:],params.lon1):find_ind(loni[0,:],params.lon2)+1]\n",
    "        #print(lat)\n",
    "        #print(lon)\n",
    "        #old_grid_data=fileobj.variables['zos'][file_time_index,:,:]\n",
    "        old_grid_data=fileobj.variables['sossheig'][file_time_index,:,:]\n",
    "        XI, YI = np.meshgrid(lon,lat)\n",
    "        \n",
    "        #interp\n",
    "        #TS: don't need to interpolate so take straight from old_grid_data \n",
    "        #etamask=interpolate.griddata((loni.flatten(),lati.flatten()),old_grid_data.flatten() , (XI,YI),method='nearest')\n",
    "\n",
    "        #eta=interpolate.griddata((loni.flatten(),lati.flatten()),old_grid_data.flatten() , (XI,YI),method='cubic')\n",
    "        \n",
    "        #TS: my change\n",
    "        eta = old_grid_data[find_ind(lati[:,0],params.lat1):find_ind(lati[:,0],params.lat2)+1,find_ind(loni[0,:],\n",
    "                                                                params.lon1):find_ind(loni[0,:],params.lon2)+1]\n",
    "        \n",
    "        #set mask\n",
    "        #TS: won't be any 0\n",
    "        #eta[np.where(etamask==0)]=0\n",
    "        eta_miss=[0]\n",
    "\n",
    "    return eta, eta_miss[0]\n",
    "\n",
    "\n",
    "\n",
    "def quick_plot(field,findrange=False):\n",
    "    '''\n",
    "    Create quick interactive diagnostic plot to double check eddy_detection is doing what we want...\n",
    "    '''\n",
    "    y,x=np.meshgrid(np.arange(field.shape[1]),np.arange(field.shape[0]))\n",
    "    plt.clf()\n",
    "\n",
    "    if not findrange:\n",
    "        plt.contourf(y, x, field, levels=np.arange(-2.5,2.5,0.05))\n",
    "    else:\n",
    "        if np.isnan(np.sum(field)):\n",
    "            plotfield=np.nan_to_num(field)\n",
    "            print('range of field is:')\n",
    "            print('min',np.min(plotfield))\n",
    "            print('max',np.max(plotfield))\n",
    "\n",
    "            plt.contourf(y, x, field,levels=np.linspace(np.min(plotfield),np.max(plotfield),50))\n",
    "\n",
    "        else:\n",
    "            print('range of field is:')\n",
    "            print('min',np.min(field))\n",
    "            print('max',np.max(field))\n",
    "\n",
    "            plt.contourf(y, x, field,levels=np.linspace(np.min(field),np.max(field),50))\n",
    "    plt.title('diagnostic plot')\n",
    "    plt.show()\n",
    "    import ipdb; ipdb.set_trace()\n",
    "\n",
    "def remove_missing(field, missing, replacement):\n",
    "    '''\n",
    "    Replaces all instances of 'missing' in 'field' with 'replacement'\n",
    "    '''\n",
    "\n",
    "    field[field==missing] = replacement\n",
    "\n",
    "    return field\n",
    "\n",
    "\n",
    "def interp_nans(data, indices):\n",
    "    '''\n",
    "    Linearly interpolates over missing values (np.nan's) in data\n",
    "    Data is defined at locations in vector indices.\n",
    "    '''\n",
    "\n",
    "    not_nan = np.logical_not(np.isnan(data))\n",
    "\n",
    "    return np.interp(indices, indices[not_nan], data[not_nan])\n",
    "\n",
    "\n",
    "def match_missing(data1, data2):\n",
    "    '''\n",
    "    Make all locations that are missing in data2 also missing in data1\n",
    "    Missing values are assumed to be np.nan.\n",
    "    '''\n",
    "\n",
    "    data1[np.isnan(data2)] = np.nan\n",
    "    return data1\n",
    "\n",
    "\n",
    "def spatial_filter(field, lon, lat, res, cut_lon, cut_lat):\n",
    "    '''\n",
    "    Performs a spatial filter, removing all features with\n",
    "    wavelenth scales larger than cut_lon in longitude and\n",
    "    cut_lat in latitude from field (defined in grid given\n",
    "    by lon and lat).  Field has spatial resolution of res\n",
    "    and land identified by np.nan's\n",
    "    '''\n",
    "    #adding res_lat because of non-uniform lat grid\n",
    "    #it is the average spacing of the grid from -60 to -37\n",
    "    res_lat = 0.053870249539613724\n",
    "    \n",
    "    field_filt = np.zeros(field.shape)\n",
    "\n",
    "    # see Chelton et al, Prog. Ocean., 2011 for explanation of factor of 1/5\n",
    "    sig_lon = (cut_lon/5.) / res\n",
    "    sig_lat = (cut_lat/5.) / res_lat\n",
    "\n",
    "    land = np.isnan(field)\n",
    "    field[land] = nanmean(field)\n",
    "\n",
    "    field_filt = field - ndimage.gaussian_filter(field, [sig_lat, sig_lon])\n",
    "\n",
    "    field_filt[land] = np.nan\n",
    "\n",
    "    return field_filt\n",
    "\n",
    "@njit\n",
    "def radians2(coord):\n",
    "    '''TS: Problem with np.radians rounding off the value\n",
    "    very sensitive to decimal place values up to 9/10\n",
    "    so using this instead and hoping it works better'''\n",
    "    rad = coord*np.pi/180\n",
    "    return rad\n",
    "\n",
    "@njit\n",
    "def distance_matrix(lons,lats):\n",
    "    '''Calculates the distances (in km) between any two cities based on the formulas\n",
    "    c = sin(lati1)*sin(lati2)+cos(longi1-longi2)*cos(lati1)*cos(lati2)\n",
    "    d = EARTH_RADIUS*Arccos(c)\n",
    "    where EARTH_RADIUS is in km and the angles are in radians.\n",
    "    https://gis.stackexchange.com/questions/4906/\n",
    "    why-is-law-of-cosines-more-preferable-than-haversine-when-calculating-distance-b/\n",
    "    9197#comment11102_4906 \n",
    "    TS: made changes np.radians converted to radians2'''\n",
    "\n",
    "    EARTH_RADIUS = 6378.1\n",
    "    X = len(lons)\n",
    "    Y = len(lats)\n",
    "    assert X == Y, 'lons and lats must have same number of elements'\n",
    "\n",
    "    d = np.zeros((X,X))\n",
    "\n",
    "    #Populate the matrix.\n",
    "    #spherical law of cosines = haversine (mathematically)\n",
    "    for i2 in range(len(lons)):\n",
    "        lati2 = lats[i2]\n",
    "        loni2 = lons[i2]\n",
    "        c = np.sin(radians2(lats)) * np.sin(radians2(lati2)) + \\\n",
    "            np.cos(radians2(lons-loni2)) * \\\n",
    "            np.cos(radians2(lats)) * np.cos(radians2(lati2))\n",
    "        \n",
    "        #to avoid rounding errors which can happen so leave them as 0\n",
    "        d[c<1,i2] = EARTH_RADIUS * np.arccos(c[c<1])\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b19aed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def distance_sloc(lat1,lat2,lon1,lon2):\n",
    "    EARTH_RADIUS = 6378.1\n",
    "    c = np.sin(radians2(lat1)) * np.sin(radians2(lat2)) + \\\n",
    "        np.cos(radians2(lon1-lon2)) * \\\n",
    "        np.cos(radians2(lat1)) * np.cos(radians2(lat2))\n",
    "    \n",
    "    d = EARTH_RADIUS * np.arccos(c)\n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d42978c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@njit\n",
    "def detect_eddies(relvort, field, lon, lat, res, Npix_min, Npix_max, amp_thresh, d_thresh, cyc='anticyclonic'):\n",
    "    len_deg_lat = 111.325 # length of 1 degree of latitude [km]\n",
    "\n",
    "    llon, llat = np.meshgrid(lon, lat)\n",
    "\n",
    "    lon_eddies = np.array([])\n",
    "    lat_eddies = np.array([])\n",
    "    amp_eddies = np.array([])\n",
    "    area_eddies = np.array([])\n",
    "    scale_eddies = np.array([])\n",
    "    circ_eddies = np.array([])\n",
    "\n",
    "    # 1. Find all regions with relvort greater (less than) 0 for anticyclonic (cyclonic) eddies \n",
    "    if cyc == 'anticyclonic':\n",
    "        regions, nregions = ndimage.label( (relvort > 0).astype(int) )\n",
    "    elif cyc == 'cyclonic':\n",
    "        regions, nregions = ndimage.label( (relvort < 0).astype(int) )\n",
    "\n",
    "    for iregion in range(nregions):\n",
    "        #print(\"iregion: \", iregion)\n",
    "   \n",
    "    # 2. Calculate number of pixels comprising detected region, reject if not within [Npix_min, Npix_max]\n",
    "        region = (regions==iregion+1).astype(int)\n",
    "        region_Npix = region.sum()\n",
    "        #print(\"Npix: \", region_Npix)\n",
    "        eddy_area_within_limits = (region_Npix < Npix_max) * (region_Npix > Npix_min)\n",
    "        #print(eddy_area_within_limits)\n",
    "\n",
    "\n",
    "\n",
    "    # 3. Detect presence of local maximum (minimum) for anticylonic (cyclonic) eddies, reject if non-existent\n",
    "        interior = ndimage.binary_erosion(region)\n",
    "        #exterior = region.astype(bool) - interior\n",
    "        #problem with exterior line above won't allow bool - bool so kept as 1/0\n",
    "        exterior = region - interior.astype(int)\n",
    "        #need to change exterior to T/F instead of 1 and 0\n",
    "        exterior = exterior.astype(bool)\n",
    "\n",
    "        if interior.sum() == 0:\n",
    "            continue\n",
    "        if cyc == 'anticyclonic':\n",
    "            has_internal_ext = field[interior].max() > field[exterior].max()\n",
    "        elif cyc == 'cyclonic':\n",
    "            has_internal_ext = field[interior].min() < field[exterior].min()\n",
    "\n",
    "    # 4. Find amplitude of region, reject if < amp_thresh\n",
    "        if cyc == 'anticyclonic':\n",
    "            amp = field[interior].max() - field[exterior].mean()\n",
    "        elif cyc == 'cyclonic':\n",
    "            amp = field[exterior].mean() - field[interior].min()\n",
    "        is_tall_eddy = amp >= amp_thresh\n",
    "\n",
    "    # 5. Find maximum linear dimension of region, reject if < d_thresh\n",
    "        #removed * has_internal_ext TS\n",
    "        if np.logical_not(eddy_area_within_limits * has_internal_ext * is_tall_eddy):\n",
    "        #if np.logical_not( eddy_area_within_limits * has_internal_ext):\n",
    "            continue\n",
    "\n",
    "        lon_ext = llon[exterior]\n",
    "        lat_ext = llat[exterior]\n",
    "\n",
    "        d = distance_matrix(lon_ext, lat_ext)\n",
    "        #print(\"dmax: \" ,d.max())\n",
    "        is_small_eddy = d.max() < d_thresh\n",
    "        \n",
    "        #removed * has_internal_ext TS\n",
    "        if eddy_area_within_limits * has_internal_ext * is_tall_eddy * is_small_eddy:\n",
    "        #if eddy_area_within_limits * has_internal_ext * is_small_eddy:\n",
    "\n",
    "                eddy_object_with_mass = rv_mask[tt] * region\n",
    "                #make all nan = 0 but we don't have nan, only zero\n",
    "                eddy_object_with_mass[np.isnan(eddy_object_with_mass)] = 0\n",
    "                j_cen, i_cen = ndimage.center_of_mass(eddy_object_with_mass)\n",
    "                lon_cen = np.interp(i_cen, range(0,len(lon)), lon)\n",
    "                lat_cen = np.interp(j_cen, range(0,len(lat)), lat)\n",
    "                lon_eddies = np.append(lon_eddies, lon_cen)\n",
    "                lat_eddies = np.append(lat_eddies, lat_cen)\n",
    "                # assign (and calculated) amplitude, area, and scale of eddies\n",
    "                amp_eddies = np.append(amp_eddies, amp)\n",
    "                #change the area because our lat grid is different (same km distance for lons and lats)\n",
    "                #area = region_Npix * res**2 * len_deg_lat * len_deg_lon(lat_cen) # [km**2]\n",
    "                area = region_Npix*(len_deg_lon(lat_cen)*res)**2\n",
    "                area_eddies = np.append(area_eddies, area)\n",
    "                scale = np.sqrt(area / np.pi) # [km]\n",
    "                scale_eddies = np.append(scale_eddies, scale)\n",
    "                \n",
    "                #circularity\n",
    "                lat_buff = np.ceil(10*scale/len_deg_lat)/10\n",
    "                lon_buff = np.ceil(10*scale/len_deg_lon(lat_cen))/10\n",
    "\n",
    "                lats_search = latsRVx[find_ind(latsRVx[:,0],lat_cen - lat_buff):find_ind(latsRVx[:,0],lat_cen+lat_buff)+1\n",
    "                              ,find_ind(lonsRVx[0,:],lon_cen-lon_buff):find_ind(lonsRVx[0,:],lon_cen+lon_buff)+1]\n",
    "\n",
    "                lons_search = lonsRVx[find_ind(latsRVx[:,0],lat_cen - lat_buff):find_ind(latsRVx[:,0],lat_cen+lat_buff)+1\n",
    "                              ,find_ind(lonsRVx[0,:],lon_cen-lon_buff):find_ind(lonsRVx[0,:],lon_cen+lon_buff)+1]\n",
    "\n",
    "                #find distances from centre\n",
    "                dist_matrix = distance_sloc(lat_cen,lats_search,lon_cen,lons_search)\n",
    "                #mask circle\n",
    "                circle_mask = np.where(dist_matrix>scale,0,1)\n",
    "                #eddy mask\n",
    "                eddy_mask = np.where(regions[find_ind(latsRVx[:,0],lat_cen - lat_buff):find_ind(latsRVx[:,0],lat_cen+lat_buff)+1\n",
    "                              ,find_ind(lonsRVx[0,:],lon_cen-lon_buff):find_ind(lonsRVx[0,:],lon_cen+lon_buff)+1]==iregion+1,1,0)\n",
    "\n",
    "                #overlap\n",
    "                overlap = eddy_mask*circle_mask\n",
    "\n",
    "                circ_idx = overlap.sum()/region_Npix\n",
    "                circ_eddies = np.append(circ_eddies,circ_idx)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #removed the mask for ssh_crits\n",
    "                \n",
    "    return lon_eddies, lat_eddies, amp_eddies, area_eddies, scale_eddies, circ_eddies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4207ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_plot(tt,lon,lat,eta,eta_filt,anticyc_eddies,cyc_eddies,ptype,plot_dir,findrange=True):\n",
    "    \"\"\"function to plot how the eddy detection alogirthm went\n",
    "    \n",
    "    :tt\n",
    "    :lon\n",
    "    :lat\n",
    "    :eta\n",
    "    :eta_filt\n",
    "    :anticyc_eddies\n",
    "    :cyc_eddies\n",
    "    :ptype\n",
    "    :plot_dir\n",
    "    :findrange=True\n",
    "    :returns: @todo\n",
    "    \"\"\"\n",
    "    def plot_eddies():\n",
    "        \"\"\"@todo: Docstring for plot_eddies\n",
    "        :returns: @todo\n",
    "        \"\"\"\n",
    "        ax.plot(anticyc_eddies[0], anticyc_eddies[1], 'k^')\n",
    "        ax.plot(cyc_eddies[0], cyc_eddies[1], 'kv')\n",
    "    \n",
    "        pass\n",
    "    if ptype=='single':\n",
    "        plt.close('all')\n",
    "        fig=plt.figure()\n",
    "        ax=fig.add_subplot(1, 1,1)\n",
    "\n",
    "    elif ptype=='rawtoo':\n",
    "        plt.close('all')\n",
    "        fig=plt.figure()\n",
    "\n",
    "        #width then height\n",
    "        fig=plt.figure(figsize=(12.0,5.0))\n",
    "        ax=fig.add_subplot(1, 2,1)\n",
    "\n",
    "        #ecj range...\n",
    "        #plt.contourf(lon, lat, eta_filt, levels=np.arange(-2.5,2.5,0.05))\n",
    "\n",
    "        #cb NEMO range\n",
    "        cs1=plt.contourf(lon, lat, eta_filt)\n",
    "                         #levels=np.linspace(-.817,0.5,40))\n",
    "        cbar=fig.colorbar(cs1,orientation='vertical')\n",
    "        ax.set_title('day: ' + str(tt)+' filtered ssh')\n",
    "        plot_eddies()\n",
    "        \n",
    "        ax=fig.add_subplot(1, 2,2)\n",
    "        cs1=plt.contourf(lon, lat, eta)\n",
    "                         #levels=np.linspace(-1.75,0.85,40))\n",
    "        cbar=fig.colorbar(cs1,orientation='vertical')\n",
    "        ax.set_title('day: ' + str(tt)+' raw ssh')\n",
    "        plot_eddies()\n",
    "\n",
    "\n",
    "        #determine range to plot \n",
    "        #if np.isnan(np.sum(eta_filt)):\n",
    "            #plt.contourf(lon,lat, eta_filt,levels=np.linspace(np.min(np.nan_to_num(eta_filt)),np.max(np.nan_to_num(eta_filt)),50))\n",
    "            #print np.min(np.nan_to_num(eta_filt))\n",
    "            #print np.max(np.nan_to_num(eta_filt))\n",
    "        #else:\n",
    "            #plt.contourf(lon,lat, eta_filt,levels=np.linspace(np.min(eta_filt),np.max(eta_filt),50))\n",
    "\n",
    "        #plt.clim(-0.5,0.5)\n",
    "        #plt.savefig(plot_dir+'eta_filt_' + str(tt).zfill(4) + '.png', bbox_inches=0)\n",
    "\n",
    "    pass\n",
    "\n",
    "def eddies_list(lon_eddies_a, lat_eddies_a, amp_eddies_a, area_eddies_a, scale_eddies_a, circ_eddies_a,lon_eddies_c, lat_eddies_c, amp_eddies_c, area_eddies_c, scale_eddies_c,circ_eddies_c):\n",
    "    '''\n",
    "    Creates list detected eddies\n",
    "    '''\n",
    "\n",
    "    eddies = []\n",
    "    \n",
    "    #lon_eddies_c and lat_eddies_a are same (no of timesteps)\n",
    "    for ed in range(len(lon_eddies_c)):\n",
    "        eddy_tmp = {}\n",
    "        eddy_tmp['lon'] = np.append(lon_eddies_a[ed], lon_eddies_c[ed])\n",
    "        eddy_tmp['lat'] = np.append(lat_eddies_a[ed], lat_eddies_c[ed])\n",
    "        eddy_tmp['amp'] = np.append(amp_eddies_a[ed], amp_eddies_c[ed])\n",
    "        eddy_tmp['area'] = np.append(area_eddies_a[ed], area_eddies_c[ed])\n",
    "        eddy_tmp['scale'] = np.append(scale_eddies_a[ed], scale_eddies_c[ed])\n",
    "        eddy_tmp['circ'] = np.append(circ_eddies_a[ed], circ_eddies_c[ed])\n",
    "        eddy_tmp['type'] = list(repeat('anticyclonic',len(lon_eddies_a[ed]))) + list(repeat('cyclonic',len(lon_eddies_c[ed])))\n",
    "        eddy_tmp['N'] = len(eddy_tmp['lon'])\n",
    "        eddies.append(eddy_tmp)\n",
    "\n",
    "    return eddies\n",
    "\n",
    "\n",
    "def eddies_init(det_eddies):\n",
    "    '''\n",
    "    Initializes list of eddies. The ith element of output is\n",
    "    a dictionary of the ith eddy containing information about\n",
    "    position and size as a function of time, as well as type.\n",
    "    '''\n",
    "\n",
    "    eddies = []\n",
    "\n",
    "    for ed in range(det_eddies[0]['N']):\n",
    "        eddy_tmp = {}\n",
    "        eddy_tmp['lon'] = np.array([det_eddies[0]['lon'][ed]])\n",
    "        eddy_tmp['lat'] = np.array([det_eddies[0]['lat'][ed]])\n",
    "        eddy_tmp['amp'] = np.array([det_eddies[0]['amp'][ed]])\n",
    "        eddy_tmp['area'] = np.array([det_eddies[0]['area'][ed]])\n",
    "        eddy_tmp['scale'] = np.array([det_eddies[0]['scale'][ed]])\n",
    "        eddy_tmp['type'] = det_eddies[0]['type'][ed]\n",
    "        eddy_tmp['time'] = np.array([1])\n",
    "        eddy_tmp['exist_at_start'] = True\n",
    "        eddy_tmp['terminated'] = False\n",
    "        eddies.append(eddy_tmp)\n",
    "\n",
    "    return eddies\n",
    "\n",
    "\n",
    "def load_rossrad():\n",
    "    '''\n",
    "    Load first baroclinic wave speed [m/s] and Rossby radius\n",
    "    of deformation [km] data from rossrad.dat (Chelton et al., 1998)\n",
    "\n",
    "    Also calculated is the first baroclinic Rossby wave speed [m/s]\n",
    "    according to the formula:  cR = -beta rossby_rad**2\n",
    "    '''\n",
    "\n",
    "    #data = np.loadtxt('data/rossrad.dat')\n",
    "\n",
    "    #cb\n",
    "    data = np.loadtxt('./rossrad.dat')\n",
    "\n",
    "    rossrad = {}\n",
    "    rossrad['lat'] = data[:,0]\n",
    "    rossrad['lon'] = data[:,1]\n",
    "    rossrad['c1'] = data[:,2] # m/s\n",
    "    rossrad['rossby_rad'] = data[:,3] # km\n",
    "\n",
    "    R = 6371.e3 # Radius of Earth [m]\n",
    "    Sigma = 2 * np.pi / (24*60*60) # Rotation frequency of Earth [rad/s]\n",
    "    beta = (2*Sigma/R) * np.cos(rossrad['lat']*np.pi/180) # 1 / m s\n",
    "    rossrad['cR'] = -beta * (1e3*rossrad['rossby_rad'])**2\n",
    "\n",
    "    return rossrad\n",
    "\n",
    "\n",
    "def is_in_ellipse(x0, y0, dE, d, x, y):\n",
    "    '''\n",
    "    Check if point (x,y) is contained in ellipse given by the equation\n",
    "\n",
    "      (x-x1)**2     (y-y1)**2\n",
    "      ---------  +  ---------  =  1\n",
    "         a**2          b**2\n",
    "\n",
    "    where:\n",
    "\n",
    "      a = 0.5 * (dE + dW)\n",
    "      b = dE\n",
    "      x1 = x0 + 0.5 * (dE - dW)\n",
    "      y1 = y0\n",
    "    '''\n",
    "\n",
    "    dW = np.max([d, dE])\n",
    "\n",
    "    b = dE\n",
    "    a = 0.5 * (dE + dW)\n",
    "\n",
    "    x1 = x0 + 0.5*(dE - dW)\n",
    "    y1 = y0\n",
    "\n",
    "    return (x-x1)**2 / a**2 + (y-y1)**2 / b**2 <= 1\n",
    "\n",
    "\n",
    "def len_deg_lon(lat):\n",
    "    '''\n",
    "    Returns the length of one degree of longitude (at latitude\n",
    "    specified) in km.\n",
    "    '''\n",
    "\n",
    "    R = 6371. # Radius of Earth [km]\n",
    "\n",
    "    return (np.pi/180.) * R * np.cos( lat * np.pi/180. )\n",
    "\n",
    "\n",
    "def calculate_d(dE, lon, lat, rossrad, dt):\n",
    "    '''\n",
    "    Calculates length of search area to the west of central point.\n",
    "    This is equal to the length of the search area to the east of\n",
    "    central point (dE) unless in the tropics ( abs(lat) < 18 deg )\n",
    "    in which case the distance a Rossby wave travels in one time step\n",
    "    (dt, days) is used instead.\n",
    "    '''\n",
    "\n",
    "    if np.abs(lat) < 18 :\n",
    "        # Rossby wave speed [km/day]\n",
    "        c = interpolate.griddata(np.array([rossrad['lon'], rossrad['lat']]).T, rossrad['cR'], (lon, lat), method='linear') * 86400. / 1000.\n",
    "        d = np.abs(1.75 * c * dt)\n",
    "    else:\n",
    "        d = dE\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def track_eddies(eddies, det_eddies, tt, dt, dt_aviso, dE_aviso, rossrad, eddy_scale_min, eddy_scale_max):\n",
    "    '''\n",
    "    Given a map of detected eddies as a function of time (det_eddies)\n",
    "    this function will update tracks of individual eddies at time step\n",
    "    tt in variable eddies\n",
    "    '''\n",
    "\n",
    "    # List of unassigned eddies at time tt\n",
    "\n",
    "    unassigned = range(det_eddies[tt]['N'])\n",
    "\n",
    "    # For each existing eddy (t<tt) loop through unassigned eddies and assign to existing eddy if appropriate\n",
    "\n",
    "    for ed in range(len(eddies)):\n",
    "\n",
    "        # Check if eddy has already been terminated\n",
    "\n",
    "        if not eddies[ed]['terminated']:\n",
    "\n",
    "            # Define search region around centroid of existing eddy ed at last known position\n",
    "    \n",
    "            x0 = eddies[ed]['lon'][-1] # [deg. lon]\n",
    "            y0 = eddies[ed]['lat'][-1] # [deg. lat]\n",
    "            dE = dE_aviso/(dt_aviso/dt) # [km]\n",
    "            d = calculate_d(dE, x0, y0, rossrad, dt) # [km]\n",
    "    \n",
    "            # Find all eddy centroids in search region at time tt\n",
    "    \n",
    "            is_near = is_in_ellipse(x0, y0, dE/len_deg_lon(y0), d/len_deg_lon(y0), det_eddies[tt]['lon'][unassigned], det_eddies[tt]['lat'][unassigned])\n",
    "    \n",
    "            # Check if eddies' amp  and area are between 0.25 and 2.5 of original eddy\n",
    "    \n",
    "            amp = eddies[ed]['amp'][-1]\n",
    "            area = eddies[ed]['area'][-1]\n",
    "            is_similar_amp = (det_eddies[tt]['amp'][unassigned] < amp*eddy_scale_max) * (det_eddies[tt]['amp'][unassigned] > amp*eddy_scale_min)\n",
    "            is_similar_area = (det_eddies[tt]['area'][unassigned] < area*eddy_scale_max) * (det_eddies[tt]['area'][unassigned] > area*eddy_scale_min)\n",
    "    \n",
    "            # Check if eddies' type is the same as original eddy\n",
    "    \n",
    "            is_same_type = np.array([det_eddies[tt]['type'][i] == eddies[ed]['type'] for i in unassigned])\n",
    "    \n",
    "            # Possible eddies are those which are near, of the right amplitude, and of the same type\n",
    "    \n",
    "            possibles = is_near * is_similar_amp * is_similar_area * is_same_type\n",
    "            if possibles.sum() > 0:\n",
    "    \n",
    "                # Of all found eddies, accept only the nearest one\n",
    "    \n",
    "                dist = np.sqrt((x0-det_eddies[tt]['lon'][unassigned])**2 + (y0-det_eddies[tt]['lat'][unassigned])**2)\n",
    "                nearest = dist == dist[possibles].min()\n",
    "                next_eddy = unassigned[np.where(nearest * possibles)[0][0]]\n",
    "    \n",
    "                # Add coordinatse and properties of accepted eddy to trajectory of eddy ed\n",
    "    \n",
    "                eddies[ed]['lon'] = np.append(eddies[ed]['lon'], det_eddies[tt]['lon'][next_eddy])\n",
    "                eddies[ed]['lat'] = np.append(eddies[ed]['lat'], det_eddies[tt]['lat'][next_eddy])\n",
    "                eddies[ed]['amp'] = np.append(eddies[ed]['amp'], det_eddies[tt]['amp'][next_eddy])\n",
    "                eddies[ed]['area'] = np.append(eddies[ed]['area'], det_eddies[tt]['area'][next_eddy])\n",
    "                eddies[ed]['scale'] = np.append(eddies[ed]['scale'], det_eddies[tt]['scale'][next_eddy])\n",
    "                eddies[ed]['time'] = np.append(eddies[ed]['time'], tt+1)\n",
    "    \n",
    "                # Remove detected eddy from list of eddies available for assigment to existing trajectories\n",
    "    \n",
    "                unassigned.remove(next_eddy)\n",
    "\n",
    "            # Terminate eddy otherwise\n",
    "\n",
    "            else:\n",
    "\n",
    "                eddies[ed]['terminated'] = True\n",
    "\n",
    "    # Create \"new eddies\" from list of eddies not assigned to existing trajectories\n",
    "\n",
    "    if len(unassigned) > 0:\n",
    "\n",
    "        for un in unassigned:\n",
    "\n",
    "            eddy_tmp = {}\n",
    "            eddy_tmp['lon'] = np.array([det_eddies[tt]['lon'][un]])\n",
    "            eddy_tmp['lat'] = np.array([det_eddies[tt]['lat'][un]])\n",
    "            eddy_tmp['amp'] = np.array([det_eddies[tt]['amp'][un]])\n",
    "            eddy_tmp['area'] = np.array([det_eddies[tt]['area'][un]])\n",
    "            eddy_tmp['scale'] = np.array([det_eddies[tt]['scale'][un]])\n",
    "            eddy_tmp['type'] = det_eddies[tt]['type'][un]\n",
    "            eddy_tmp['time'] = np.array([tt+1])\n",
    "            eddy_tmp['exist_at_start'] = False\n",
    "            eddy_tmp['terminated'] = False\n",
    "            eddies.append(eddy_tmp)\n",
    "\n",
    "    return eddies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27e6d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "latsRVx = latsRVx.values\n",
    "lonsRVx = lonsRVx.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb6f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb60ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0ec76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f7556b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122fe82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8f906d5",
   "metadata": {},
   "source": [
    "## Eddy Detection Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea8dd8",
   "metadata": {},
   "source": [
    "Adjust parameters in params.py: T, res, dt, pathroot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6045ef9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eddy detection started\n",
      "number of time steps to loop over:  365\n",
      "timestep:  1 . out of:  365\n",
      "timestep:  2 . out of:  365\n",
      "timestep:  3 . out of:  365\n",
      "timestep:  4 . out of:  365\n",
      "timestep:  5 . out of:  365\n",
      "timestep:  6 . out of:  365\n",
      "timestep:  7 . out of:  365\n",
      "timestep:  8 . out of:  365\n",
      "timestep:  9 . out of:  365\n",
      "timestep:  10 . out of:  365\n",
      "timestep:  11 . out of:  365\n",
      "timestep:  12 . out of:  365\n",
      "timestep:  13 . out of:  365\n",
      "timestep:  14 . out of:  365\n",
      "timestep:  15 . out of:  365\n",
      "timestep:  16 . out of:  365\n",
      "timestep:  17 . out of:  365\n",
      "timestep:  18 . out of:  365\n",
      "timestep:  19 . out of:  365\n",
      "timestep:  20 . out of:  365\n",
      "timestep:  21 . out of:  365\n",
      "timestep:  22 . out of:  365\n",
      "timestep:  23 . out of:  365\n",
      "timestep:  24 . out of:  365\n",
      "timestep:  25 . out of:  365\n",
      "timestep:  26 . out of:  365\n",
      "timestep:  27 . out of:  365\n",
      "timestep:  28 . out of:  365\n",
      "timestep:  29 . out of:  365\n",
      "timestep:  30 . out of:  365\n",
      "timestep:  31 . out of:  365\n",
      "timestep:  32 . out of:  365\n",
      "timestep:  33 . out of:  365\n",
      "timestep:  34 . out of:  365\n",
      "timestep:  35 . out of:  365\n",
      "timestep:  36 . out of:  365\n",
      "timestep:  37 . out of:  365\n",
      "timestep:  38 . out of:  365\n",
      "timestep:  39 . out of:  365\n",
      "timestep:  40 . out of:  365\n",
      "timestep:  41 . out of:  365\n",
      "timestep:  42 . out of:  365\n",
      "timestep:  43 . out of:  365\n",
      "timestep:  44 . out of:  365\n",
      "timestep:  45 . out of:  365\n",
      "timestep:  46 . out of:  365\n",
      "timestep:  47 . out of:  365\n",
      "timestep:  48 . out of:  365\n",
      "timestep:  49 . out of:  365\n",
      "timestep:  50 . out of:  365\n",
      "timestep:  51 . out of:  365\n",
      "timestep:  52 . out of:  365\n",
      "timestep:  53 . out of:  365\n",
      "timestep:  54 . out of:  365\n",
      "timestep:  55 . out of:  365\n",
      "timestep:  56 . out of:  365\n",
      "timestep:  57 . out of:  365\n",
      "timestep:  58 . out of:  365\n",
      "timestep:  59 . out of:  365\n",
      "timestep:  60 . out of:  365\n",
      "timestep:  61 . out of:  365\n",
      "timestep:  62 . out of:  365\n",
      "timestep:  63 . out of:  365\n",
      "timestep:  64 . out of:  365\n",
      "timestep:  65 . out of:  365\n",
      "timestep:  66 . out of:  365\n",
      "timestep:  67 . out of:  365\n",
      "timestep:  68 . out of:  365\n",
      "timestep:  69 . out of:  365\n",
      "timestep:  70 . out of:  365\n",
      "timestep:  71 . out of:  365\n",
      "timestep:  72 . out of:  365\n",
      "timestep:  73 . out of:  365\n",
      "timestep:  74 . out of:  365\n",
      "timestep:  75 . out of:  365\n",
      "timestep:  76 . out of:  365\n",
      "timestep:  77 . out of:  365\n",
      "timestep:  78 . out of:  365\n",
      "timestep:  79 . out of:  365\n",
      "timestep:  80 . out of:  365\n",
      "timestep:  81 . out of:  365\n",
      "timestep:  82 . out of:  365\n",
      "timestep:  83 . out of:  365\n",
      "timestep:  84 . out of:  365\n",
      "timestep:  85 . out of:  365\n",
      "timestep:  86 . out of:  365\n",
      "timestep:  87 . out of:  365\n",
      "timestep:  88 . out of:  365\n",
      "timestep:  89 . out of:  365\n",
      "timestep:  90 . out of:  365\n",
      "timestep:  91 . out of:  365\n",
      "timestep:  92 . out of:  365\n",
      "timestep:  93 . out of:  365\n",
      "timestep:  94 . out of:  365\n",
      "timestep:  95 . out of:  365\n",
      "timestep:  96 . out of:  365\n",
      "timestep:  97 . out of:  365\n",
      "timestep:  98 . out of:  365\n",
      "timestep:  99 . out of:  365\n",
      "timestep:  100 . out of:  365\n",
      "timestep:  101 . out of:  365\n",
      "timestep:  102 . out of:  365\n",
      "timestep:  103 . out of:  365\n",
      "timestep:  104 . out of:  365\n",
      "timestep:  105 . out of:  365\n",
      "timestep:  106 . out of:  365\n",
      "timestep:  107 . out of:  365\n",
      "timestep:  108 . out of:  365\n",
      "timestep:  109 . out of:  365\n",
      "timestep:  110 . out of:  365\n",
      "timestep:  111 . out of:  365\n",
      "timestep:  112 . out of:  365\n",
      "timestep:  113 . out of:  365\n",
      "timestep:  114 . out of:  365\n",
      "timestep:  115 . out of:  365\n",
      "timestep:  116 . out of:  365\n",
      "timestep:  117 . out of:  365\n",
      "timestep:  118 . out of:  365\n",
      "timestep:  119 . out of:  365\n",
      "timestep:  120 . out of:  365\n",
      "timestep:  121 . out of:  365\n",
      "timestep:  122 . out of:  365\n",
      "timestep:  123 . out of:  365\n",
      "timestep:  124 . out of:  365\n",
      "timestep:  125 . out of:  365\n",
      "timestep:  126 . out of:  365\n",
      "timestep:  127 . out of:  365\n",
      "timestep:  128 . out of:  365\n",
      "timestep:  129 . out of:  365\n",
      "timestep:  130 . out of:  365\n",
      "timestep:  131 . out of:  365\n",
      "timestep:  132 . out of:  365\n",
      "timestep:  133 . out of:  365\n",
      "timestep:  134 . out of:  365\n",
      "timestep:  135 . out of:  365\n",
      "timestep:  136 . out of:  365\n",
      "timestep:  137 . out of:  365\n",
      "timestep:  138 . out of:  365\n",
      "timestep:  139 . out of:  365\n",
      "timestep:  140 . out of:  365\n",
      "timestep:  141 . out of:  365\n",
      "timestep:  142 . out of:  365\n",
      "timestep:  143 . out of:  365\n",
      "timestep:  144 . out of:  365\n",
      "timestep:  145 . out of:  365\n",
      "timestep:  146 . out of:  365\n",
      "timestep:  147 . out of:  365\n",
      "timestep:  148 . out of:  365\n",
      "timestep:  149 . out of:  365\n",
      "timestep:  150 . out of:  365\n",
      "timestep:  151 . out of:  365\n",
      "timestep:  152 . out of:  365\n",
      "timestep:  153 . out of:  365\n",
      "timestep:  154 . out of:  365\n",
      "timestep:  155 . out of:  365\n",
      "timestep:  156 . out of:  365\n",
      "timestep:  157 . out of:  365\n",
      "timestep:  158 . out of:  365\n",
      "timestep:  159 . out of:  365\n",
      "timestep:  160 . out of:  365\n",
      "timestep:  161 . out of:  365\n",
      "timestep:  162 . out of:  365\n",
      "timestep:  163 . out of:  365\n",
      "timestep:  164 . out of:  365\n",
      "timestep:  165 . out of:  365\n",
      "timestep:  166 . out of:  365\n",
      "timestep:  167 . out of:  365\n",
      "timestep:  168 . out of:  365\n",
      "timestep:  169 . out of:  365\n",
      "timestep:  170 . out of:  365\n",
      "timestep:  171 . out of:  365\n",
      "timestep:  172 . out of:  365\n",
      "timestep:  173 . out of:  365\n",
      "timestep:  174 . out of:  365\n",
      "timestep:  175 . out of:  365\n",
      "timestep:  176 . out of:  365\n",
      "timestep:  177 . out of:  365\n",
      "timestep:  178 . out of:  365\n",
      "timestep:  179 . out of:  365\n",
      "timestep:  180 . out of:  365\n",
      "timestep:  181 . out of:  365\n",
      "timestep:  182 . out of:  365\n",
      "timestep:  183 . out of:  365\n",
      "timestep:  184 . out of:  365\n",
      "timestep:  185 . out of:  365\n",
      "timestep:  186 . out of:  365\n",
      "timestep:  187 . out of:  365\n",
      "timestep:  188 . out of:  365\n",
      "timestep:  189 . out of:  365\n",
      "timestep:  190 . out of:  365\n",
      "timestep:  191 . out of:  365\n",
      "timestep:  192 . out of:  365\n",
      "timestep:  193 . out of:  365\n",
      "timestep:  194 . out of:  365\n",
      "timestep:  195 . out of:  365\n",
      "timestep:  196 . out of:  365\n",
      "timestep:  197 . out of:  365\n",
      "timestep:  198 . out of:  365\n",
      "timestep:  199 . out of:  365\n",
      "timestep:  200 . out of:  365\n",
      "timestep:  201 . out of:  365\n",
      "timestep:  202 . out of:  365\n",
      "timestep:  203 . out of:  365\n",
      "timestep:  204 . out of:  365\n",
      "timestep:  205 . out of:  365\n",
      "timestep:  206 . out of:  365\n",
      "timestep:  207 . out of:  365\n",
      "timestep:  208 . out of:  365\n",
      "timestep:  209 . out of:  365\n",
      "timestep:  210 . out of:  365\n",
      "timestep:  211 . out of:  365\n",
      "timestep:  212 . out of:  365\n",
      "timestep:  213 . out of:  365\n",
      "timestep:  214 . out of:  365\n",
      "timestep:  215 . out of:  365\n",
      "timestep:  216 . out of:  365\n",
      "timestep:  217 . out of:  365\n",
      "timestep:  218 . out of:  365\n",
      "timestep:  219 . out of:  365\n",
      "timestep:  220 . out of:  365\n",
      "timestep:  221 . out of:  365\n",
      "timestep:  222 . out of:  365\n",
      "timestep:  223 . out of:  365\n",
      "timestep:  224 . out of:  365\n",
      "timestep:  225 . out of:  365\n",
      "timestep:  226 . out of:  365\n",
      "timestep:  227 . out of:  365\n",
      "timestep:  228 . out of:  365\n",
      "timestep:  229 . out of:  365\n",
      "timestep:  230 . out of:  365\n",
      "timestep:  231 . out of:  365\n",
      "timestep:  232 . out of:  365\n",
      "timestep:  233 . out of:  365\n",
      "timestep:  234 . out of:  365\n",
      "timestep:  235 . out of:  365\n",
      "timestep:  236 . out of:  365\n",
      "timestep:  237 . out of:  365\n",
      "timestep:  238 . out of:  365\n",
      "timestep:  239 . out of:  365\n",
      "timestep:  240 . out of:  365\n",
      "timestep:  241 . out of:  365\n",
      "timestep:  242 . out of:  365\n",
      "timestep:  243 . out of:  365\n",
      "timestep:  244 . out of:  365\n",
      "timestep:  245 . out of:  365\n",
      "timestep:  246 . out of:  365\n",
      "timestep:  247 . out of:  365\n",
      "timestep:  248 . out of:  365\n",
      "timestep:  249 . out of:  365\n",
      "timestep:  250 . out of:  365\n",
      "timestep:  251 . out of:  365\n",
      "timestep:  252 . out of:  365\n",
      "timestep:  253 . out of:  365\n",
      "timestep:  254 . out of:  365\n",
      "timestep:  255 . out of:  365\n",
      "timestep:  256 . out of:  365\n",
      "timestep:  257 . out of:  365\n",
      "timestep:  258 . out of:  365\n",
      "timestep:  259 . out of:  365\n",
      "timestep:  260 . out of:  365\n",
      "timestep:  261 . out of:  365\n",
      "timestep:  262 . out of:  365\n",
      "timestep:  263 . out of:  365\n",
      "timestep:  264 . out of:  365\n",
      "timestep:  265 . out of:  365\n",
      "timestep:  266 . out of:  365\n",
      "timestep:  267 . out of:  365\n",
      "timestep:  268 . out of:  365\n",
      "timestep:  269 . out of:  365\n",
      "timestep:  270 . out of:  365\n",
      "timestep:  271 . out of:  365\n",
      "timestep:  272 . out of:  365\n",
      "timestep:  273 . out of:  365\n",
      "timestep:  274 . out of:  365\n",
      "timestep:  275 . out of:  365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep:  276 . out of:  365\n",
      "timestep:  277 . out of:  365\n",
      "timestep:  278 . out of:  365\n",
      "timestep:  279 . out of:  365\n",
      "timestep:  280 . out of:  365\n",
      "timestep:  281 . out of:  365\n",
      "timestep:  282 . out of:  365\n",
      "timestep:  283 . out of:  365\n",
      "timestep:  284 . out of:  365\n",
      "timestep:  285 . out of:  365\n",
      "timestep:  286 . out of:  365\n",
      "timestep:  287 . out of:  365\n",
      "timestep:  288 . out of:  365\n",
      "timestep:  289 . out of:  365\n",
      "timestep:  290 . out of:  365\n",
      "timestep:  291 . out of:  365\n",
      "timestep:  292 . out of:  365\n",
      "timestep:  293 . out of:  365\n",
      "timestep:  294 . out of:  365\n",
      "timestep:  295 . out of:  365\n",
      "timestep:  296 . out of:  365\n",
      "timestep:  297 . out of:  365\n",
      "timestep:  298 . out of:  365\n",
      "timestep:  299 . out of:  365\n",
      "timestep:  300 . out of:  365\n",
      "timestep:  301 . out of:  365\n",
      "timestep:  302 . out of:  365\n",
      "timestep:  303 . out of:  365\n",
      "timestep:  304 . out of:  365\n",
      "timestep:  305 . out of:  365\n",
      "timestep:  306 . out of:  365\n",
      "timestep:  307 . out of:  365\n",
      "timestep:  308 . out of:  365\n",
      "timestep:  309 . out of:  365\n",
      "timestep:  310 . out of:  365\n",
      "timestep:  311 . out of:  365\n",
      "timestep:  312 . out of:  365\n",
      "timestep:  313 . out of:  365\n",
      "timestep:  314 . out of:  365\n",
      "timestep:  315 . out of:  365\n",
      "timestep:  316 . out of:  365\n",
      "timestep:  317 . out of:  365\n",
      "timestep:  318 . out of:  365\n",
      "timestep:  319 . out of:  365\n",
      "timestep:  320 . out of:  365\n",
      "timestep:  321 . out of:  365\n",
      "timestep:  322 . out of:  365\n",
      "timestep:  323 . out of:  365\n",
      "timestep:  324 . out of:  365\n",
      "timestep:  325 . out of:  365\n",
      "timestep:  326 . out of:  365\n",
      "timestep:  327 . out of:  365\n",
      "timestep:  328 . out of:  365\n",
      "timestep:  329 . out of:  365\n",
      "timestep:  330 . out of:  365\n",
      "timestep:  331 . out of:  365\n",
      "timestep:  332 . out of:  365\n",
      "timestep:  333 . out of:  365\n",
      "timestep:  334 . out of:  365\n",
      "timestep:  335 . out of:  365\n",
      "timestep:  336 . out of:  365\n",
      "timestep:  337 . out of:  365\n",
      "timestep:  338 . out of:  365\n",
      "timestep:  339 . out of:  365\n",
      "timestep:  340 . out of:  365\n",
      "timestep:  341 . out of:  365\n",
      "timestep:  342 . out of:  365\n",
      "timestep:  343 . out of:  365\n",
      "timestep:  344 . out of:  365\n",
      "timestep:  345 . out of:  365\n",
      "timestep:  346 . out of:  365\n",
      "timestep:  347 . out of:  365\n",
      "timestep:  348 . out of:  365\n",
      "timestep:  349 . out of:  365\n",
      "timestep:  350 . out of:  365\n",
      "timestep:  351 . out of:  365\n",
      "timestep:  352 . out of:  365\n",
      "timestep:  353 . out of:  365\n",
      "timestep:  354 . out of:  365\n",
      "timestep:  355 . out of:  365\n",
      "timestep:  356 . out of:  365\n",
      "timestep:  357 . out of:  365\n",
      "timestep:  358 . out of:  365\n",
      "timestep:  359 . out of:  365\n",
      "timestep:  360 . out of:  365\n",
      "timestep:  361 . out of:  365\n",
      "timestep:  362 . out of:  365\n",
      "timestep:  363 . out of:  365\n",
      "timestep:  364 . out of:  365\n",
      "timestep:  365 . out of:  365\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "eddies_list() takes 10 positional arguments but 12 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:110\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: eddies_list() takes 10 positional arguments but 12 were given"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Eddy Detection Algorithm\n",
    "\n",
    "'''\n",
    "\n",
    "  Software for the tracking of eddies in\n",
    "  OFAM model output following Chelton et\n",
    "  al., Progress in Oceanography, 2011.\n",
    "\n",
    "'''\n",
    "\n",
    "# Load required modules\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "# Turn the followin on if you are running on storm sometimes - Forces matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#import eddy_functions as eddy\n",
    "\n",
    "# Load parameters\n",
    "\n",
    "from params import *\n",
    "#TS - overwrite params T\n",
    "\n",
    "T = 365\n",
    "# Load latitude and longitude vectors and restrict to domain of interest\n",
    "\n",
    "# lon, lat = eddy.load_lonlat(run)\n",
    "lon, lat = load_lonlat(run)\n",
    "##chris' dodgy hack for not having the eric find_nearest function...\n",
    "#i1=0\n",
    "#i2=2000\n",
    "#j1=0\n",
    "#j2=2000\n",
    "# lon, lat, i1, i2, j1, j2 = eddy.restrict_lonlat(lon, lat, lon1, lon2, lat1, lat2)\n",
    "lon, lat, i1, i2, j1, j2 = restrict_lonlat(lon, lat, lon1, lon2, lat1, lat2)\n",
    "\n",
    "# Loop over time\n",
    "\n",
    "\n",
    "lon_eddies_a = []\n",
    "lat_eddies_a = []\n",
    "amp_eddies_a = []\n",
    "area_eddies_a = []\n",
    "scale_eddies_a = []\n",
    "circ_eddies_a = []\n",
    "lon_eddies_c = []\n",
    "lat_eddies_c = []\n",
    "amp_eddies_c = []\n",
    "area_eddies_c = []\n",
    "scale_eddies_c = []\n",
    "circ_eddies_c= []\n",
    "\n",
    "print ('eddy detection started')\n",
    "print (\"number of time steps to loop over: \",T)\n",
    "for tt in range(T):\n",
    "    print (\"timestep: \",tt+1,\". out of: \", T)\n",
    "\n",
    "    # Load map of sea surface height (SSH)\n",
    " \n",
    "    #eta, eta_miss = eddy.load_eta(run, tt, i1, i2, j1, j2)\n",
    "    eta, eta_miss = load_eta(run, tt, i1, i2, j1, j2)\n",
    "    #eta = eddy.remove_missing(eta, missing=eta_miss, replacement=np.nan)\n",
    "    \n",
    "    #changed eta in that code so should be fine\n",
    "    eta = remove_missing(eta, missing=eta_miss, replacement=np.nan)\n",
    "    #eddy.quick_plot(eta,findrange=True)\n",
    "    # \n",
    "    ## Spatially filter SSH field\n",
    "    # \n",
    "    #eta_filt = eddy.spatial_filter(eta, lon, lat, res, cut_lon, cut_lat)\n",
    "    eta_filt = spatial_filter(eta, lon, lat, res, cut_lon, cut_lat)\n",
    "    #eddy.quick_plot(eta_filt,findrange=True)\n",
    "    # \n",
    "    ## Detect lon and lat coordinates of eddies\n",
    "    #\n",
    "    #lon_eddies, lat_eddies, amp, area, scale = eddy.detect_eddies(eta_filt, lon, lat, ssh_crits, res, Npix_min, Npix_max, amp_thresh, d_thresh, cyc='anticyclonic')\n",
    "    \n",
    "    #removed ssh_crits and added rv_mask\n",
    "    lon_eddies, lat_eddies, amp, area, scale, circ = detect_eddies(rv_mask[tt], eta_filt, lon, lat, res, Npix_min, Npix_max, amp_thresh, d_thresh, cyc='anticyclonic')\n",
    "    lon_eddies_a.append(lon_eddies)\n",
    "    lat_eddies_a.append(lat_eddies)\n",
    "    amp_eddies_a.append(amp)\n",
    "    area_eddies_a.append(area)\n",
    "    scale_eddies_a.append(scale)\n",
    "    circ_eddies_a.append(circ)\n",
    "\n",
    "    #lon_eddies, lat_eddies, amp, area, scale = eddy.detect_eddies(eta_filt, lon, lat, ssh_crits, res, Npix_min, Npix_max, amp_thresh, d_thresh, cyc='cyclonic')\n",
    "    lon_eddies, lat_eddies, amp, area, scale, circ = detect_eddies(rv_mask[tt], eta_filt, lon, lat,  res, Npix_min, Npix_max, amp_thresh, d_thresh, cyc='cyclonic')\n",
    "    lon_eddies_c.append(lon_eddies)\n",
    "    lat_eddies_c.append(lat_eddies)\n",
    "    amp_eddies_c.append(amp)\n",
    "    area_eddies_c.append(area)\n",
    "    scale_eddies_c.append(scale)\n",
    "    circ_eddies_c.append(circ)\n",
    " \n",
    "    # Plot map of filtered SSH field\n",
    "\n",
    "    eddies_a=(lon_eddies_a[tt],lat_eddies_a[tt])\n",
    "    eddies_c=(lon_eddies_c[tt],lat_eddies_c[tt])\n",
    "    #eddy.detection_plot(tt,lon,lat,eta,eta_filt,eddies_a,eddies_c,'rawtoo',plot_dir,findrange=False)\n",
    "    detection_plot(tt,lon,lat,eta,eta_filt,eddies_a,eddies_c,'rawtoo',plot_dir,findrange=False)\n",
    "\n",
    "# Combine eddy information from all days into a list\n",
    "\n",
    "#eddies = eddy.eddies_list(lon_eddies_a, lat_eddies_a, amp_eddies_a, area_eddies_a, scale_eddies_a, lon_eddies_c, lat_eddies_c, amp_eddies_c, area_eddies_c, scale_eddies_c)\n",
    "eddies = eddies_list(lon_eddies_a, lat_eddies_a, amp_eddies_a, area_eddies_a, scale_eddies_a,circ_eddies_a, lon_eddies_c, lat_eddies_c, amp_eddies_c, area_eddies_c, scale_eddies_c, circ_eddies_c)\n",
    "\n",
    "#turn on/off save\n",
    "np.savez(data_dir+'eddy_det_BP12-CNCLNG01_2005_2009new', eddies=eddies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c04585f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eddies = eddies_list(lon_eddies_a, lat_eddies_a, amp_eddies_a, area_eddies_a, scale_eddies_a,circ_eddies_a, lon_eddies_c, lat_eddies_c, amp_eddies_c, area_eddies_c, scale_eddies_c, circ_eddies_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fba6279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/home/tsmith/scratch/Okubo/BP12-CNCLNG01/depth/Eddy_Detect/Eddy_detect_BP12-CNCLNG01_2005_2009new', eddies=eddies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the eddy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90d5da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eddy_det = np.load('/home/tsmith/scratch/Okubo/BP12-CNCLNG01/depth/Eddy_Detect/Eddy_detect_BP12-CNCLNG01_2005_2009new.npz',allow_pickle=True)\n",
    "\n",
    "eddy_array = eddy_det['eddies']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f63f35f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eddy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fa66da25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26150"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eddy_sum = 0\n",
    "for i in range(len(eddy_array)):\n",
    "    num = eddy_array[i]['N']\n",
    "    eddy_sum += num\n",
    "\n",
    "eddy_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fc0f3",
   "metadata": {},
   "source": [
    "### Find Big Eddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3297b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of max scale\n",
    "max_scale = []\n",
    "for i in range(len(eddy_array)):\n",
    "        max_scale += [np.max(eddy_array[i]['scale'])]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c9614883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bf594cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = max_scale.index(np.max(max_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ce75519f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3aac2abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113.35060496437889"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_scale[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7afea2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed = find_ind(eddy_array[t]['scale'],max_scale[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "009621dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7451f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
